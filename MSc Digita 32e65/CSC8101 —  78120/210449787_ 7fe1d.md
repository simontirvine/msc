# 210449787_SIrvine_CSC8101_Spark

# [CSC8101] Big Data Analytics - 2022 Spark Coursework

## Goals

The goal of the assignment is to use Spark to program a set of tasks against the well-known datataset ofÂ [NYC Taxi rides dataset](https://www.google.com/url?q=https%3A%2F%2Fwww1.nyc.gov%2Fsite%2Ftlc%2Fabout%2Ftlc-trip-record-data.page&sa=D&sntz=1&usg=AFQjCNFWV1YQUeDO4BuLp3Ef-5mPuFYNrQ), and benchmark their performance (execution time) at different data scales.

You are given increasingly large fragments of the dataset, indicated using "t-shirt size" notation as S, M, L, XL, XXL.Â  Your goal is to program the tasks so that they exploit the implicit parallelism of the Spark engine, which runs on a cluster, so that they scale with the size of the input.

In practice, you should find that the tasks execution times are proportional to the number of records in the dataset, or put another way, that the ratio of execution time to record numbers is approximately constant.

Thus, your implementation is assessed both on (1) correctness and (2) scalability.

To achieve this, you will work wither with DatabricksÂ  clusters on Microsoft Azure

## Coursework results

Analyse the resulting execution times and comment on the effect of dataset size, dataset format and task complexity (with and without task 1.2) on pipeline performance.

<aside>
ðŸ’¡ *Table 1. Pipeline performance for `parquet` format.*

| metric | S | M | L |
| --- | --- | --- | --- |
| rows (M) | 2.9 | 15.6 | 42.0 |
| execution time (w/o 1.2) | 26.4 | 243.6 | 2823.0 |
| execution time | 33.4 | 417.0 | 4248.0 |
| sec / 1M records (w/o 1.2) | 9.1 | 15.5 | 67.2 |
| sec / 1M records | 11.5 | 26.7 | 101.1 |
</aside>

<aside>
ðŸ’¡ *Table 2. Pipeline performance for `delta` format.*

| metric | S | M | L | XL | XXL |
| --- | --- | --- | --- | --- | --- |
| rows (M) | 2.9 | 15.6 | 42 | 90.4 | 132.4 |
| execution time (w/o 1.2) | 23.6 | 63 | 163.2 | 375 | 574.8 |
| execution time | 32.2 | 85.8 | 219 | 499.8 | 754.8 |
| sec / 1M records (w/o 1.2) | 8.1 | 4.0 | 3.9 | 4.1 | 4.3 |
| sec / 1M records | 11.1 | 5.5 | 5.2 | 5.5 | 5.7 |
</aside>

### Comments

- *`Parquet`*and `*delta`* formats have a significant impact on the performance of python scripts running on Spark.
- On the whole, `*delta`* formats performed much better with lower execution times - ranging from equivalent to 100 times faster. As can be seen from the results tables above, as the dataset size increases there is a greater disparity in performance across the two formats.
- *`Parquet`*format applied to large datasets (sizes L, XL and XXL and from 40GB) resulted in several errors and failed tasks due to resourcing constraints (in red font).
- *`Parquet`*format execution times were indirectly proportional to the dataset size, i.e. the bigger the dataset, the higher the sec / 1M record figure. However, `*delta`* formats ran fairly consistently across all dataset sizes.
- **Insight:** For datasets greater than 5GB it is worth formatting as `*delta` .*

---

### Attached notebook

[210449787_SIrvine_CSC8101_spark_notebook_Rev0.ipynb](210449787_%207fe1d/210449787_SIrvine_CSC8101_spark_notebook_Rev0.ipynb)

[210449787_SIrvine_CSC8101_spark_notebook_Rev0.html](210449787_%207fe1d/210449787_SIrvine_CSC8101_spark_notebook_Rev0.html)

---

## Notebook

### Inputs

- **NYC Taxi Trips dataset**Â - list of recorded taxi trips, each with several characteristics, namely: distance, number of passengers, origin zone, destination zone and trip cost (total amount charged to customer).
- **NYC Zones dataset**Â - list of zones wherein trips can originate/terminate.

### Tasks

1. Data cleaning
    1. Remove "0 distance" and 'no passengers' records.
    2. Remove outlier records.
2. Add new columns
    1. Join with zones dataset
    2. Compute the unit profitability of each trip
3. Zone summarisation and ranking
    1. Summarise trip data per zone
    2. Obtain the top 10 ranks according to:
        1. The total trip volume
        2. Their average profitabilitiy
        3. The total passenger volume
4. Record the total and task-specific execution times for each dataset size and format.

### How to

### Code structure and implementation

- You must implement your solution to each task in the provided function code skeleton.
- The task-specific functions are combined together to form the full pipeline code, executed last (do not modify this code).
- Before implementing the specified function skeleton, you should develop and test your solution on separate code cells (create and destroy cells as needed).

### Development

- Develop an initial working solution for the 'S' dataset and only then optimise it for larger dataset sizes.
- To perform vectorised operations on a DataFrame:
    - use the API docs to look for existing vectorised functions in:Â [https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions)
    - if a customised function is required (e.g. to add a new column based on a linear combination of other columns), implement your own User Defined Function (UDF). See:Â [https://spark.apache.org/docs/latest/sql-ref-functions-udf-scalar.html](https://spark.apache.org/docs/latest/sql-ref-functions-udf-scalar.html)
- Use only theÂ `pyspark.sql`Â API - documentation link below - (note that searching through the docs returns results from theÂ `pyspark.sql`Â API together with theÂ `pyspark.pandas`Â API):
    - [https://spark.apache.org/docs/3.2.0/api/python/reference/pyspark.sql.html](https://spark.apache.org/docs/3.2.0/api/python/reference/pyspark.sql.html)
- Periodically download your notebook to your computer as backup and safety measure against accidental file deletion.

### Execution time measurement

- Execution time is calculated and returned by the Spark Engine and shown in the output region of the cell.
- To measure the execution time of a task you must perform aÂ `collect`Â or similar operation (e.g.Â `take`) on the returned DataFrame.

## Task 0 - Read data

The code below is ready to run.Â **Do not modify this code**. It does the following:

- Reads the 'zones' dataset into variable 'zone_names'
- Defines theÂ `init_trips`Â function that allows you to read the 'trips' dataset (from the DBFS FileStore) given the dataset size ('S' to 'XXL') and format ('parquet' or 'delta') as function arguments
- Defines theÂ `pipeline`Â function, called in Task 4 to measure the execution time of the entire data processing pipeline
- Shows you how to call theÂ `init_trips`Â function and display dataset characteristics (number of rows, schema)

```python
## global imports

try:
    from pyspark.sql.functions import *
    from pyspark.sql import SparkSession
    from pyspark.sql.types import *
    import pandas as pd
# Additional global imports
    import pyspark.sql as ps
    import pyspark.sql.functions as pf
    spark = SparkSession.builder\
   .getOrCreate()
    
except Exception as err:
    print(f'Import error{err}')

# Load zone names dataset - (much faster to read small file from git than dbfs)
zones_file_url = 'https://raw.githubusercontent.com/NewcastleComputingScience/csc8101-coursework/main/02-assignment-spark/taxi_zone_names.csv'
zone_names = spark.createDataFrame(pd.read_csv(zones_file_url))

# Function to load trips dataset by selected dataset size
def init_trips(size = 'S', data_format = "parquet", taxi_folder = "/FileStore/tables/taxi"):     
    
    files = {
        'S'  : ['2021_07'],
        'M'  : ['2021'],
        'L'  : ['2020_21'],
        'XL' : ['1_6_2019', '7_12_2019'],
        'XXL': ['1_6_2019', '7_12_2019', '2020_21']
    }
    
    # validate input dataset size
    if size not in files.keys():
        print("Invalid input dataset size. Must be one of {}".format(list(files.keys())))
        return None               
    
    if data_format == "parquet":
        filenames = list(map(lambda s: f'{taxi_folder}/tripdata_{s}.parquet', files[size]))
        trips_df = spark.read.parquet(filenames[0])
        
        for name in filenames[1:]:
            trips_df = trips_df.union(spark.read.parquet(name))
            
    elif data_format == "delta":
        filenames = f"{taxi_folder}/taxi-{size}-delta/"
        trips_df = spark.read.format("delta").load(filenames)
    
    else:
        print("Invalid data format. Must be one of {}".format(['parquet', 'delta']))
        return None
        
    print(
    """
    Trips dataset loaded!
    ---
      Size: {s}
      Format: {f}
      Tables loaded: {ds}
      Number of trips (dataset rows): {tc:,}
    """.format(s = size, f = data_format, ds = filenames, tc = trips_df.count()))
    
    return trips_df

# helper function to print dataset row count
def print_count(df):
    print("Row count: {t:,}".format(t = df.count()))

def pipeline(trips_df, with_task_12 = False, zones_df = zone_names):
    # Do not edit
    #---

    ## Task 1.1
    _trips_11 = t11_remove_zeros(trips_df)

    ## Task 1.2
    if with_task_12:
        _trips_12 = t12_remove_outliers(_trips_11)
    else:
        _trips_12 = _trips_11

    ## Task 2.1
    _trips_21 = t21_join_zones(_trips_12, zones_df = zone_names)

    ## Task 2.2
    _trips_22 = t22_calc_profit(_trips_21)

    ## Task 3.1
    _graph = t31_summarise_trips(_trips_22)

    ## Task 3.2
    _zones = t32_summarise_zones_pairs(_graph)

    _top10_trips     = t32_top10_trips(_zones)
    _top10_profit    = t32_top10_profit(_zones)
    _top10_passenger = t32_top10_passenger(_zones)
    
    return([_top10_trips, _top10_profit, _top10_passenger])
```

```python
# CHANGE the value of argument 'size' to record the pipeline execution times for increasing dataset sizes
SIZE = 'S'
DATA_FORMAT = 'parquet'

# Load trips dataset
trips = init_trips(SIZE, DATA_FORMAT)

# uncomment line only for small datasets
# trips.take(1)
```

```python
print_count(trips)
```

```python
# dataset schemas
trips.printSchema()
```

```python
display(trips[['PULocationID', 'DOLocationID', 'trip_distance', 'passenger_count', 'total_amount']].take(5))
```

```python
zone_names.printSchema()
```

```python
display(zone_names.take(5))
```

## Task 1 - Filter rows

**Input:**Â trips dataset

### Task 1.1 - Remove "0 distance" and 'no passengers' records

Remove dataset rows that represent invalid trips:

- Trips whereÂ `trip_distance == 0`Â (no distance travelled)
- Trips whereÂ `passenger_count == 0`Â andÂ `total_amount == 0`Â (we want to retain records whereÂ `total_amount`Â > 0 - these may be significant as the taxi may have carried some parcel, for example)

Altogether, a record is removed if it satisfies the following conditions:

`trip_distance == 0`Â orÂ `(passenger_count == 0`Â andÂ `total_amount == 0)`.

**Recommended:**Â Select only the relevant dataset columns for this and subsequent tasks:Â `['PULocationID', 'DOLocationID', 'trip_distance', 'passenger_count', 'total_amount')]`

### Task 1.2 - Remove outliers using the modified z-score

Despite having removed spurious "zero passengers" trips in task 1.1, columnsÂ `total_amount`Â andÂ `trip_distance`Â contain additional outlier values that must be identified and removed.

To identify and remove outliers, you will use the modifiedÂ [z-score](https://en.wikipedia.org/wiki/Standard_score)Â method. The modified z-score uses the median andÂ [Median Absolute Deviation](https://en.wikipedia.org/wiki/Median_absolute_deviation)Â (MAD), instead of the mean and standard deviation, to determine how far an observation (indexed by i) is from the mean:

$$
z_i=\frac{x_iâˆ’median(x)}{\bm {MAD}}
$$

where x represents the input vector, xi is an element of x and zi is its corresponding z-score. In turn, the MAD formula is:

$$
\bm {MAD}=1.483 âˆ— median(|x_iâˆ’median(x)|)
$$

Observations withÂ **high**Â (absolute) z-score are considered outlier observations. A score is consideredÂ **high**Â if itsÂ **absolute z-score**Â is larger than a threshold T = 3.5:

$$
|z_i|>3.5
$$

where T represents the number of unit standard deviations beyond which a score is considered an outlier ([wiki](https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule)).

This process is repeated twice, once for each of the columnsÂ `total_amount`Â andÂ `trip_distance`Â (in any order).

**Important:**Â Use the surrogate functionÂ `[percentile_approx](https://spark.apache.org/docs/3.2.0/api/python/reference/api/pyspark.sql.functions.percentile_approx.html?highlight=percentile#pyspark.sql.functions.percentile_approx)`Â to estimate the median (calculating the median values for a column is expensive as it cannot be parallelised efficiently).

```python
# develop your solution here (create/destroy cells as needed) and then implement it in the functions below
```

```python
####################################################################################################################
######################################## Solution implementation to task 1.1 #######################################
####################################################################################################################

### selecting necessary features
cols = ['PULocationID', 'DOLocationID', 'trip_distance', 'passenger_count', 'total_amount']
trips = trips.select(cols)

def t11_remove_zeros(df):
    remove_0_records = df.where('trip_distance != 0').where('passenger_count != 0')\
    .where('total_amount !=0')
 
    return remove_0_records
```

```python
# execute task 1.1
trips_11 = t11_remove_zeros(trips)

print_count(trips_11)

## uncomment only for smaller datasets
# display(trips_11.take(10))
```

```python
####################################################################################################################
######################################## Solution implementation to task 1.2 #######################################
####################################################################################################################

def t12_remove_outliers(df):
    
    MAD_distance = df.groupby('PULocationID').agg(expr('percentile(trip_distance, array(0.5))')[0].alias('median'))\
            .join(trips_11, "PULocationID", "left").withColumn("difference_median",
            abs(col('trip_distance')- col('median'))).groupby('PULocationID', 'median')\
            .agg(expr('percentile(difference_median, array(0.5))')[0].alias('median_absolute_difference'))\
            .withColumn('MAD',col('median_absolute_difference')*1.483)
    
    MAD_amount = df.groupby('PULocationID').agg(expr('percentile(total_amount, array(0.5))')[0].alias('amount_median'))\
            .join(trips_11, "PULocationID", "left").withColumn("amount_difference_median",
            abs(col('total_amount')- col('amount_median'))).groupby('PULocationID', 'amount_median')\
            .agg(expr('percentile(amount_difference_median, array(0.5))')[0].alias('amount_median_absolute_difference'))\
            .withColumn('MAD_amount',col('amount_median_absolute_difference')*1.483)

    outliersremoved_distance = df.join(MAD_distance, "PULocationID", "left")\
        .filter(abs(col("trip_distance")- col("median")) <= (col("MAD")*3.5)).select(cols)
    
    outliersremoved_amount = df.join(MAD_amount, "PULocationID", "left")\
        .filter(abs(col("total_amount")- col("amount_median")) <= (col("MAD_amount")*3.5)).select(cols)
    
    outliersremoved = outliersremoved_distance.join(outliersremoved_amount,
                                   ['PULocationID','DOLocationID','trip_distance','passenger_count','total_amount'],'inner').drop_duplicates()

    return outliersremoved
```

```python
# execute task 1.2
trips_12 = t12_remove_outliers(trips_11)

print_count(trips_12)
# display(trips_12.take(10))
```

## Task 2 - Compute new columns

### Task 2.1 - Zone names

Obtain theÂ **start**Â andÂ **end**Â zone names of each trip by joining theÂ `trips`Â andÂ `zone_names`Â datasets (i.e. by using theÂ `zone_names`Â dataset as lookup table).

**Note:**Â The columns containing the start and end zone ids of each trip are namedÂ `PULocationID`Â andÂ `DOLocationID`, respectively.

### Task 2.2 - Unit profitability

Compute the columnÂ `unit_profitabilty = total_amount / trip_distance`.

```python
####################################################################################################################
######################################## Solution implementation to task 2.1 #######################################
####################################################################################################################

def t21_join_zones(df, zones_df = zone_names):
    # input: output of task 1.2 and zone_names dataset
    
    joined = df.join(zones_df.select('LocationID','Zone'),df.PULocationID == zones_df.LocationID,'inner')\
                .withColumnRenamed('Zone','PUzone').drop('LocationID')\
                .join(zones_df.select('LocationID','Zone'),df.DOLocationID == zones_df.LocationID,'inner')\
                .withColumnRenamed('Zone','DOzone')
    
    return joined
```

```python
# execute task 2.1
trips_21 = t21_join_zones(trips_12, zones_df = zone_names)

print_count(trips_21)
# display(trips_21.take(10))
```

```python
####################################################################################################################
######################################## Solution implementation to task 2.2 #######################################
####################################################################################################################

def t22_calc_profit(df):
    # input: output of task 2.1
    unitProf = trips_21.withColumn('unit_profitability',round(col('total_amount')/col('trip_distance'),2))
    return unitProf
```

```python
# execute task 2.2
trips_22 = t22_calc_profit(trips_21)

print_count(trips_22)
# display(trips_22.take(10))
```

## Task 3: Rank zones by traffic, passenger volume and profitability

### 3.1 - Summarise interzonal travel

Build a graph data structure of zone-to-zone traffic, representing aggregated data about trips between any two zones. The graph will have one node for each zone and one edge connecting each pair of zones. In addition, edges contain aggregate information about all trips between those zones.

For example, zones Z1 and Z2 are connected byÂ *two*Â edges: edge Z1 --> Z2 carries aggregate data about all trips that originated in Z1 and ended in Z2, and edge Z2 --> Z2 carries aggregate data about all trips that originated in Z2 and ended in Z1.

The aggregate information of interzonal travel must include the following data:

- `average_unit_profit`Â - the average unit profitability (calculated asÂ `mean(unit_profitabilty)`).
- `trips_count`Â -- the total number of recorded trips.
- `total_passengers`Â -- the total number of passenger across all trips (sum ofÂ `passenger_count`).

This graph can be represented as a new dataframe, with schema:

[`PULocationID`,Â `DOLocationID`,Â `average_unit_profit`,Â `trips_count`,Â `total_passengers`Â ]

**hint**: theÂ `groupby()`Â operator produces aÂ `pyspark.sql.GroupedData`Â structure. You can then calculate multiple aggregations from this usingÂ `pyspark.sql.GroupedData.agg()`:

- [https://spark.apache.org/docs/3.2.0/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.groupby.html](https://spark.apache.org/docs/3.2.0/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.groupby.html)
- [https://spark.apache.org/docs/3.2.0/api/python/reference/api/pyspark.sql.GroupedData.agg.html](https://spark.apache.org/docs/3.2.0/api/python/reference/api/pyspark.sql.GroupedData.agg.html)

### Task 3.2 - Obtain top-10 zones

For each of the following measures, report the top-10 zonesÂ *using their plain names you dereferenced in the previous step, not the codes*. Note that this requires ranking the nodes in different orders. Specifically, you need to calculate the following further aggregations:

- theÂ **total**Â number of trips originating from Z. This is simply the sum ofÂ `trips_count`Â over all outgoing edges for Z, i.e., edges of the form Z -> *
- theÂ **average**Â profitability of a zone. This is the average of allÂ `average_unit_profit`Â over allÂ *outgoing*Â edges from Z.
- TheÂ **total**Â passenger volume measured as theÂ **sum**Â ofÂ `total_passengers`Â carried in trips that originate from Z

```python
####################################################################################################################
######################################## Solution implementation to task 3.1 #######################################
####################################################################################################################

def t31_summarise_trips(df):
    
    interzonal_travel = (df
    .groupBy("PULocationID", "DOLocationID")
    .agg(mean('unit_profitability').alias('average_unit_profit'),
        count('*').alias('total_trips'),
        sum('passenger_count').alias('total_passengers'))
    )
    return interzonal_travel
```

```python
# execute task 3.1
graph = t31_summarise_trips(trips_22)

print_count(graph)
# display(graph.take(10))
```

```python
####################################################################################################################
######################################## Solution implementation to task 3.2 #######################################
####################################################################################################################

def t32_summarise_zones_pairs(df, zones_df = zone_names):
    
    df_zones = df.groupby('PULocationID').sum()\
            .join(zones_df,df.PULocationID == zones_df.LocationID, "left")

    return df_zones

# Top 10 ranked zones by traffic (trip volume)
def t32_top10_trips(df_zones):
    # input: output of task 3.2
    
    df_zones = df_zones.sort('sum(total_trips)',ascending = False)\
                        .select('Zone','sum(total_trips)').limit(10)
    return df_zones

# Top 10 ranked zones by profit
def t32_top10_profit(df_zones):
    # input: output of task 3.2
    
    df_zones = df_zones.sort('sum(average_unit_profit)',ascending = False)\
                                .select('Zone','sum(average_unit_profit)').limit(10)
    return df_zones

# Top 10 ranked zones by passenger volume
def t32_top10_passenger(df_zones):
    # input: output of task 3.2
   
    df_zones = df_zones.sort('sum(total_passengers)',ascending = False)\
                                .select('Zone','sum(total_passengers)').limit(10)
    return df_zones
```

```python
# execute task 3.2
zones = t32_summarise_zones_pairs(graph)

top10_trips     = t32_top10_trips(zones)
top10_profit    = t32_top10_profit(zones)
top10_passenger = t32_top10_passenger(zones)
```

```python
# use 'display()' or return a pandas DataFrame for 'pretty' output
display(top10_trips.toPandas())
```

```python
# use 'display()' return a pandas DataFrame for 'pretty' output
display(top10_profit.withColumn('sum(average_unit_profit)',round(col('sum(average_unit_profit)'),2)).toPandas())
```

```python
# use 'display()' or return a pandas DataFrame for 'pretty' output
display(top10_passenger.toPandas())
```

## Task 4 - Record the pipeline's execution time

Record the execution time of:

1. the whole pipeline
2. the whole pipeline except task 1.2

on the two tables below, for all dataset sizes:Â `'S'`,Â `'M'`,Â `'L'`,Â `'XL'`,Â `'XXL'`, and data formats:Â `parquet`Â andÂ `delta`.

Analyse the resulting execution times and comment on the effect of dataset size, dataset format and task complexity (with and without task 1.2) on pipeline performance.

```python
# after developing your solution, it may be convenient to combine all your functions in a single cell (at the start or end of the notebook)

# CHANGE the value of the following arguments to record the pipeline execution times for increasing dataset sizes
SIZE = 'S'
DATA_FORMAT = 'parquet'
WITH_TASK_12 = True

# Load trips dataset
trips = init_trips(SIZE, DATA_FORMAT)
```

```python
# run and record the resulting execution time shown by databricks (on the cell footer)

# IMPORTANT: this function calls all task functions in order of occurrence. For this code to run without errors, you have to load into memory all of the previous task-specific functions, even if you haven't implemented these yet.
pipeline(trips, with_task_12 = WITH_TASK_12)

```

## Comments

- `Parquet` and `delta` formats have a significant impact on the performance of python scripts running on Spark.
- On the whole, `delta` formats performed much better with lower execution times - ranging from equivalent to 100 times faster. As can be seen from the results tables above, as the dataset size increases there is a greater disparity in performance across the two formats.
- `Parquet` format applied to large datasets (sizes L, XL and XXL and from 40GB) resulted in several errors and failed tasks due to resourcing constraints (in **BOLD/RED** font).
- `Parquet` format execution times were indirectly proportional to the dataset size, i.e. the bigger the dataset, the higher the sec / 1M record figure. However, `delta` formats ran fairly consistently across all dataset sizes.

### Insight

For datasets greater than 5GB it is worth formatting as `delta`.

<aside>
ðŸ’¡ *Table 1. Pipeline performance for `parquet` format.*

| metric | S | M | L | XL | XXL |
| --- | --- | --- | --- | --- | --- |
| rows (M) | 2.9 | 15.6 | 42.0 | 90.4 | 132.4 |
| execution time (w/o 1.2) | 26.4 | 243.6 | 2823.0 | 7992 | 11916 |
| execution time | 33.4 | 417.0 | 4248.0 | 9612 | 13212 |
| sec / 1M records (w/o 1.2) | 9.1 | 15.5 | 67.2 | 88.4 | 90.0 |
| sec / 1M records | 11.5 | 26.7 | 101.1 | 106.3 | 99.8 |
</aside>

<aside>
ðŸ’¡ *Table 2. Pipeline performance for `delta` format.*

| metric | S | M | L | XL | XXL |
| --- | --- | --- | --- | --- | --- |
| rows (M) | 2.9 | 15.6 | 42 | 90.4 | 132.4 |
| execution time (w/o 1.2) | 23.6 | 63 | 163.2 | 375 | 574.8 |
| execution time | 32.2 | 85.8 | 219 | 499.8 | 754.8 |
| sec / 1M records (w/o 1.2) | 8.1 | 4.0 | 3.9 | 4.1 | 4.3 |
| sec / 1M records | 11.1 | 5.5 | 5.2 | 5.5 | 5.7 |
</aside>

---